{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alpha_vantage.timeseries import TimeSeries\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "#Data Preprocessing\n",
    "\n",
    "ts = TimeSeries(key='0IN175LIOUOBEICW',output_format = 'pandas')\n",
    "# Get json object with the intraday data and another with  the call's metadata\n",
    "data, meta_data = ts.get_daily_adjusted(symbol='BOX', outputsize='full')\n",
    "data = data.rename(columns = {'1. open':'Open','2. high':'High', '3. low':'Low', \n",
    "                 '4. close':'Close', '5. adjusted close':'Price', '6. volume':'Volume',\n",
    "                 '7. dividend amount':'Dividends', '8. split coefficient':'Stock Splits'})\n",
    "data = data.sort_values(by='date', ascending=False)\n",
    "#Create the additional features\n",
    "data['Yesterday_Price'] = data['Price'].shift(-1)\n",
    "data['Tomorrow_Price'] = data['Price'].shift(1)\n",
    "data['Next_Week_Price'] = data['Price'].shift(7)\n",
    "data = data[['Open', 'High', 'Low', 'Volume', 'Stock Splits','Dividends', 'Yesterday_Price', 'Price','Tomorrow_Price','Next_Week_Price']]\n",
    "today = data.head(1)\n",
    "today_date = data.index[0]\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "imp = IterativeImputer(max_iter=10, verbose =0)\n",
    "data2 = data.iloc[:,:]\n",
    "imp.fit(data2)\n",
    "df = imp.transform(data2)\n",
    "df = pd.DataFrame(df, columns=data2.columns)\n",
    "\n",
    "\n",
    "# Inialize dictionary of values\n",
    "\n",
    "from collections import defaultdict\n",
    "dd = defaultdict(lambda:dd)\n",
    "levels = dd\n",
    "levels = {\n",
    "    1 : {\n",
    "        \"Raw Data\" : {\"features\": 'X' , \"target\": 'Y'},\n",
    "        \"Train Data\" : {\"features\": 'X_train', 'train_target': 'Y_train'},\n",
    "        \"Test Data\":{'features': 'X_test', 'test_target': 'Y_test'},\n",
    "        \"Scalar\" : 'scaler', \n",
    "        \"Model\": 'mlp',\n",
    "        \"Results\" : {'train_acc' : 'acc', 'test_acc' : 'acc'},\n",
    "        'Predictions' : 'y_hat',\n",
    "    },\n",
    "    2 : {\n",
    "        \"Raw Data\" : {\"features\": 'X' , \"target\": 'Y'},\n",
    "        \"Train Data\" : {\"features\": 'X_train', 'train_target': 'Y_train'},\n",
    "        \"Test Data\":{'features': 'X_test', 'test_target': 'Y_test'},\n",
    "        \"Scalar\" : 'scaler', \n",
    "        \"Model\": 'mlp',\n",
    "        \"Results\" : {'train_acc' : 'acc', 'test_acc' : 'acc'},\n",
    "        'Predictions' : 'y_hat',\n",
    "    },\n",
    "    3 : {\n",
    "        \"Raw Data\" : {\"features\": 'X' , \"target\": 'Y'},\n",
    "        \"Train Data\" : {\"features\": 'X_train', 'train_target': 'Y_train'},\n",
    "        \"Test Data\":{'features': 'X_test', 'test_target': 'Y_test'},\n",
    "        \"Scalar\" : 'scaler', \n",
    "        \"Model\": 'mlp',\n",
    "        \"Results\" : {'train_acc' : 'acc', 'test_acc' : 'acc'},\n",
    "        'Predictions' : 'y_hat',\n",
    "}\n",
    "}\n",
    "\n",
    "\n",
    "#Split Data\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "for l in levels:\n",
    "    #split data into features and target\n",
    "    X = df.iloc[:,:-4+int(l)].astype(float)\n",
    "    y = df.iloc[:,-4+int(l)].astype(float)\n",
    "    #splite data into training and testing\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
    "    \n",
    "    #save the datasets\n",
    "    levels[l]['Raw Data']['features'] = X\n",
    "    levels[l]['Raw Data']['target'] = y\n",
    "    levels[l]['Train Data']['features'] = X_train\n",
    "    levels[l]['Test Data']['features'] = X_test\n",
    "    levels[l]['Train Data']['target'] = y_train\n",
    "    levels[l]['Test Data']['target'] = y_test\n",
    "   \n",
    " #Scale Data\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "for l in levels:\n",
    "    #iniatiliza scaler\n",
    "    scaler = StandardScaler()\n",
    "    #Pull features and scaler\n",
    "    train_features = levels[l]['Train Data']['features']\n",
    "    test_features = levels[l]['Test Data']['features']\n",
    "    \n",
    "    #Fit scalar and scale features\n",
    "    fit_scaler = scaler.fit(train_features)\n",
    "    train_features = fit_scaler.transform(train_features)\n",
    "    test_features = fit_scaler.transform(test_features)\n",
    "    \n",
    "    #update variables in dictionary \n",
    "    levels[l]['Train Data']['features'] = train_features\n",
    "    levels[l]['Scalar'] = fit_scaler\n",
    "    levels[l]['Test Data']['features'] = test_features\n",
    " \n",
    "\n",
    "\n",
    " # Build Models\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#Inialize model for each set and add to dict\n",
    "levels[1]['Model'] = MLPRegressor(max_iter = 300)\n",
    "levels[2]['Model'] = MLPRegressor(max_iter = 300)\n",
    "levels[3]['Model'] = MLPRegressor(max_iter = 300)\n",
    "#define parameter space\n",
    "parameter_space = {\n",
    "    'hidden_layer_sizes':[(100,),(100,50,25),(100,50,25,25)],\n",
    "    'alpha':[0.0001, 0.001, 0.01, 0.1, 1, 10],\n",
    "    'learning_rate_init':[0.001, 0.01, 0.1, 1],\n",
    "}\n",
    "for l in levels:\n",
    "    #Bring in required elements\n",
    "    mlp = levels[l]['Model']\n",
    "    train_features = levels[l]['Train Data']['features']\n",
    "    train_labels = levels[l]['Train Data']['target']\n",
    "    test_features = levels[l]['Test Data']['features']\n",
    "    test_labels = levels[l]['Test Data']['target']\n",
    "    #Optimize parameters and train model\n",
    "    mlp = GridSearchCV(mlp, parameter_space, n_jobs=-1, cv=5)\n",
    "    mlp_trained = mlp.fit(train_features,train_labels)\n",
    "    #Evaluate accuracy\n",
    "    train_acc, test_acc = mlp_trained.score(train_features,train_labels) , mlp_trained.score(test_features,test_labels)\n",
    "    #return model and metrics\n",
    "    levels[l]['Model'] = mlp_trained\n",
    "    levels[l]['Results']['train_acc'] = train_acc\n",
    "    levels[l]['Results']['test_acc'] = test_acc\n",
    "\n",
    "    \n",
    "# Make Predictions\n",
    "\n",
    "result = {\n",
    "    \"Today\" : levels[1]['Results'],\n",
    "    \"Tomorrow\" : levels[2]['Results'],\n",
    "    \"Next Week\" : levels[3]['Results'],\n",
    "}\n",
    "results = pd.DataFrame(result)\n",
    "\n",
    "for l in levels:\n",
    "    print(\"Level : \" , l)\n",
    "    #bring in each model and adjusted dataset\n",
    "    mlp = levels[l]['Model']\n",
    "    features = levels[l]['Raw Data']['features']\n",
    "    scalar = levels[l]['Scalar']\n",
    "    #scale the data to align with model sclaing and select first row as today\n",
    "    features_scaled = pd.DataFrame(scalar.transform(features), columns = features.columns)\n",
    "    today = features_scaled.head(1)\n",
    "    #make prediction and return it\n",
    "    prediction = mlp.predict(today)\n",
    "    levels[l]['Predictions'] = prediction.astype(float)\n",
    "    \n",
    "predictions = {\n",
    "    \"Today\" : levels[1]['Predictions'],\n",
    "    \"Tomorrow\" : levels[2]['Predictions'],\n",
    "    \"Next Week\" : levels[3]['Predictions'],\n",
    "}\n",
    "predictions = pd.DataFrame(predictions)\n",
    "\n",
    "summary = predictions.append(results, ignore_index=True)\n",
    "summary.index = ['Prediction', 'train_acc', 'test_acc']\n",
    "summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
